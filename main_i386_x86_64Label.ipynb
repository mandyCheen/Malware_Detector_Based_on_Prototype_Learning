{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading all datasets...\n",
      "train dataset shape: (820, 14)\n",
      "train dataset family number: 41\n",
      "test dataset shape: (300, 14)\n",
      "test dataset family number: 15\n",
      "val dataset shape: (100, 14)\n",
      "val dataset family number: 5\n",
      "Vectorizing byte sequence using TF-IDF.\n"
     ]
    }
   ],
   "source": [
    "from malwareDetector import malwareDetector\n",
    "from dataLoader import DatasetLoad\n",
    "from byteSequenceVectorize import ByteSequenceVectorizer\n",
    "\n",
    "detector = malwareDetector(cluster=False, train=True, val=True, cpuArch=\"i386_x86_64Label\",\n",
    "                            support_shots=5, query_shots=5, class_per_iter=20, class_per_iter_test=5,\n",
    "                            loss=\"\", dropout_prob=0.5, splitByCpu=True)\n",
    "\n",
    "dataset = DatasetLoad(detector)\n",
    "\n",
    "vectorize = ByteSequenceVectorizer(method = \"tfidf\", detector = detector, dataset=dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorize Method:  tfidf\n",
      "Loading vectorized byte sequence from ./embedding...\n"
     ]
    }
   ],
   "source": [
    "print(\"Vectorize Method: \", vectorize.vectorize_method)\n",
    "featureDim = 1000\n",
    "n_gramRange = (2, 4)\n",
    "\n",
    "train_X, test_X, val_X, train_y, test_y, val_y, train_map, test_map, val_map = vectorize.vectorize_func(featureDim, n_gramRange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train X shape:  (820, 1000) <class 'numpy.ndarray'>\n",
      "Train y shape:  (820,) <class 'numpy.ndarray'>\n",
      "Test X shape:  (300, 1000)\n",
      "Test y shape:  (300,)\n",
      "Val X shape:  (100, 1000)\n",
      "Val y shape:  (100,)\n",
      "Train Map:  {0: 'aenjaris_Advanced Micro Devices X86-64', 1: 'blueshell_Advanced Micro Devices X86-64', 2: 'bpfdoor_Advanced Micro Devices X86-64', 3: 'camelot_Advanced Micro Devices X86-64', 4: 'chisel_Advanced Micro Devices X86-64', 5: 'cleanlog_Advanced Micro Devices X86-64', 6: 'cobaltstrike_Advanced Micro Devices X86-64', 7: 'cryptonote_Advanced Micro Devices X86-64', 8: 'dnsamp_Advanced Micro Devices X86-64', 9: 'dnscat_Advanced Micro Devices X86-64', 10: 'dropperl_Advanced Micro Devices X86-64', 11: 'drtycow_Advanced Micro Devices X86-64', 12: 'exploitscan_Advanced Micro Devices X86-64', 13: 'ezuriloader_Advanced Micro Devices X86-64', 14: 'fritzfrog_Advanced Micro Devices X86-64', 15: 'fscan_Advanced Micro Devices X86-64', 16: 'gafgyt_Advanced Micro Devices X86-64', 17: 'hive_Advanced Micro Devices X86-64', 18: 'horsepill_Advanced Micro Devices X86-64', 19: 'kaiji_Advanced Micro Devices X86-64', 20: 'lady_Advanced Micro Devices X86-64', 21: 'malsource_Advanced Micro Devices X86-64', 22: 'malxmr_Advanced Micro Devices X86-64', 23: 'merlin_Advanced Micro Devices X86-64', 24: 'meterpreter_Advanced Micro Devices X86-64', 25: 'mirai_Advanced Micro Devices X86-64', 26: 'pnscan_Advanced Micro Devices X86-64', 27: 'poseidon_Advanced Micro Devices X86-64', 28: 'prochider_Advanced Micro Devices X86-64', 29: 'psybnc_Advanced Micro Devices X86-64', 30: 'rekoobe_Advanced Micro Devices X86-64', 31: 'reversessh_Advanced Micro Devices X86-64', 32: 'revproxy_Advanced Micro Devices X86-64', 33: 'sliver_Advanced Micro Devices X86-64', 34: 'sshdoor_Advanced Micro Devices X86-64', 35: 'stowaway_Advanced Micro Devices X86-64', 36: 'tsunami_Advanced Micro Devices X86-64', 37: 'vtflooder_Advanced Micro Devices X86-64', 38: 'winexe_Advanced Micro Devices X86-64', 39: 'winnti_Advanced Micro Devices X86-64', 40: 'xmrig_Advanced Micro Devices X86-64'}\n",
      "Test Map:  {0: 'backegmm_Intel 80386', 1: 'cornelgen_Intel 80386', 2: 'dnsamp_Intel 80386', 3: 'gafgyt_Intel 80386', 4: 'kaiji_Intel 80386', 5: 'meterpreter_Intel 80386', 6: 'mirai_Intel 80386', 7: 'pnscan_Intel 80386', 8: 'prochider_Intel 80386', 9: 'rkit_Intel 80386', 10: 'sckit_Intel 80386', 11: 'sshbrute_Intel 80386', 12: 'sshdoor_Intel 80386', 13: 'tsunami_Intel 80386', 14: 'xorddos_Intel 80386'}\n",
      "Val Map:  {0: 'equationdrug_Intel 80386', 1: 'exploitscan_Intel 80386', 2: 'local_Intel 80386', 3: 'race_Intel 80386', 4: 'sliver_Intel 80386'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Train X shape: \", train_X.shape, type(train_X))\n",
    "print(\"Train y shape: \", train_y.shape, type(train_y))\n",
    "print(\"Test X shape: \", test_X.shape)\n",
    "print(\"Test y shape: \", test_y.shape)\n",
    "if(val_X is not None):\n",
    "    print(\"Val X shape: \", val_X.shape)\n",
    "    print(\"Val y shape: \", val_y.shape)\n",
    "print(\"Train Map: \", train_map)\n",
    "print(\"Test Map: \", test_map)\n",
    "if(val_map is not None):\n",
    "    print(\"Val Map: \", val_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes per iteration: 20\n",
      "N-shot: 5\n",
      "=== Epoch: 0 ===\n",
      "Avg Train Loss: 2.277526767253876, Avg Train Acc: 0.2658000000566244 (Best)\n",
      "Avg Val Loss: 0.9444604307413101, Avg Val Acc: 0.6257333344221115 (Best)\n",
      "=== Epoch: 1 ===\n",
      "Avg Train Loss: 1.7476279270648956, Avg Train Acc: 0.3998999984562397 (Best: 0.6257333344221115)\n",
      "Avg Val Loss: 0.8972574883699417, Avg Val Acc: 0.5855999982357025 (Best: 0.6257333344221115)\n",
      "=== Epoch: 2 ===\n",
      "Avg Train Loss: 1.4843651175498962, Avg Train Acc: 0.47649999886751176 (Best: 0.6257333344221115)\n",
      "Avg Val Loss: 0.8787640738487243, Avg Val Acc: 0.6065333321690559 (Best: 0.6257333344221115)\n",
      "=== Epoch: 3 ===\n",
      "Avg Train Loss: 1.384161297082901, Avg Train Acc: 0.49349999547004697 (Best: 0.6257333344221115)\n",
      "Avg Val Loss: 0.8331784814596176, Avg Val Acc: 0.6130666634440423 (Best: 0.6257333344221115)\n",
      "=== Epoch: 4 ===\n",
      "Avg Train Loss: 1.2799021863937379, Avg Train Acc: 0.5262999951839447 (Best: 0.6257333344221115)\n",
      "Avg Val Loss: 0.8140273201465607, Avg Val Acc: 0.6153333309292793 (Best: 0.6257333344221115)\n",
      "=== Epoch: 5 ===\n",
      "Avg Train Loss: 1.2000116282701492, Avg Train Acc: 0.5644999998807907 (Best: 0.6257333344221115)\n",
      "Avg Val Loss: 0.8108296149969101, Avg Val Acc: 0.6121333348751068 (Best: 0.6257333344221115)\n",
      "=== Epoch: 6 ===\n",
      "Avg Train Loss: 1.1561612010002136, Avg Train Acc: 0.5730000004172325 (Best: 0.6257333344221115)\n",
      "Avg Val Loss: 0.875963625907898, Avg Val Acc: 0.6016000032424926 (Best: 0.6257333344221115)\n",
      "=== Epoch: 7 ===\n",
      "Avg Train Loss: 1.097460065484047, Avg Train Acc: 0.597699998319149 (Best: 0.6257333344221115)\n",
      "Avg Val Loss: 0.8607509994506836, Avg Val Acc: 0.6564000016450882 (Best)\n",
      "=== Epoch: 8 ===\n",
      "Avg Train Loss: 1.0690168231725692, Avg Train Acc: 0.6055000004172325 (Best: 0.6564000016450882)\n",
      "Avg Val Loss: 0.8240980285406113, Avg Val Acc: 0.6410666650533676 (Best: 0.6564000016450882)\n",
      "=== Epoch: 9 ===\n",
      "Avg Train Loss: 1.047215946316719, Avg Train Acc: 0.6156999999284745 (Best: 0.6564000016450882)\n",
      "Avg Val Loss: 0.8447821801900863, Avg Val Acc: 0.6373333343863488 (Best: 0.6564000016450882)\n",
      "=== Epoch: 10 ===\n",
      "Avg Train Loss: 1.0366085147857667, Avg Train Acc: 0.6158999979496003 (Best: 0.6564000016450882)\n",
      "Avg Val Loss: 0.8035505920648575, Avg Val Acc: 0.6370666652917862 (Best: 0.6564000016450882)\n",
      "=== Epoch: 11 ===\n",
      "Avg Train Loss: 1.0046963292360305, Avg Train Acc: 0.6268000000715256 (Best: 0.6564000016450882)\n",
      "Avg Val Loss: 0.8271694976091385, Avg Val Acc: 0.6746666669845581 (Best)\n",
      "=== Epoch: 12 ===\n",
      "Avg Train Loss: 1.0272309893369675, Avg Train Acc: 0.6179000023007393 (Best: 0.6746666669845581)\n",
      "Avg Val Loss: 0.8449275052547455, Avg Val Acc: 0.62266666918993 (Best: 0.6746666669845581)\n",
      "=== Epoch: 13 ===\n",
      "Avg Train Loss: 1.0220145297050476, Avg Train Acc: 0.6188000014424324 (Best: 0.6746666669845581)\n",
      "Avg Val Loss: 0.8662413293123246, Avg Val Acc: 0.6286666679382324 (Best: 0.6746666669845581)\n",
      "=== Epoch: 14 ===\n",
      "Avg Train Loss: 0.94759047716856, Avg Train Acc: 0.647399999499321 (Best: 0.6746666669845581)\n",
      "Avg Val Loss: 0.8310128724575043, Avg Val Acc: 0.6705333346128464 (Best: 0.6746666669845581)\n",
      "=== Epoch: 15 ===\n",
      "Avg Train Loss: 0.9666853713989257, Avg Train Acc: 0.6383000013232231 (Best: 0.6746666669845581)\n",
      "Avg Val Loss: 0.8598194032907486, Avg Val Acc: 0.638933333158493 (Best: 0.6746666669845581)\n",
      "=== Epoch: 16 ===\n",
      "Avg Train Loss: 0.9650053575634956, Avg Train Acc: 0.6408000016212463 (Best: 0.6746666669845581)\n",
      "Avg Val Loss: 0.8373949337005615, Avg Val Acc: 0.6312000000476837 (Best: 0.6746666669845581)\n",
      "=== Epoch: 17 ===\n",
      "Avg Train Loss: 0.9902694150805473, Avg Train Acc: 0.6257000002264976 (Best: 0.6746666669845581)\n",
      "Avg Val Loss: 0.8199736171960831, Avg Val Acc: 0.6099999997019768 (Best: 0.6746666669845581)\n",
      "=== Epoch: 18 ===\n",
      "Avg Train Loss: 0.9618307039141655, Avg Train Acc: 0.6394999986886978 (Best: 0.6746666669845581)\n",
      "Avg Val Loss: 0.8624590760469437, Avg Val Acc: 0.6077333307266235 (Best: 0.6746666669845581)\n",
      "=== Epoch: 19 ===\n",
      "Avg Train Loss: 0.9317087733745575, Avg Train Acc: 0.6497000017762184 (Best: 0.6746666669845581)\n",
      "Avg Val Loss: 0.8480499517917633, Avg Val Acc: 0.6113333329558372 (Best: 0.6746666669845581)\n",
      "=== Epoch: 20 ===\n",
      "Avg Train Loss: 0.905771484375, Avg Train Acc: 0.661300000846386 (Best: 0.6746666669845581)\n",
      "Avg Val Loss: 0.8533357959985733, Avg Val Acc: 0.6034666672348976 (Best: 0.6746666669845581)\n",
      "=== Epoch: 21 ===\n",
      "Avg Train Loss: 0.9313642662763596, Avg Train Acc: 0.6548000001907348 (Best: 0.6746666669845581)\n",
      "Avg Val Loss: 0.8518860918283463, Avg Val Acc: 0.6290666678547859 (Best: 0.6746666669845581)\n",
      "=== Epoch: 22 ===\n",
      "Avg Train Loss: 0.8977259123325347, Avg Train Acc: 0.6664000016450882 (Best: 0.6746666669845581)\n",
      "Avg Val Loss: 0.8485686391592026, Avg Val Acc: 0.633866668343544 (Best: 0.6746666669845581)\n",
      "=== Epoch: 23 ===\n",
      "Avg Train Loss: 0.9161514693498611, Avg Train Acc: 0.6574999976158142 (Best: 0.6746666669845581)\n",
      "Avg Val Loss: 0.8290608608722687, Avg Val Acc: 0.6463999989628791 (Best: 0.6746666669845581)\n",
      "=== Epoch: 24 ===\n",
      "Avg Train Loss: 0.8844747984409332, Avg Train Acc: 0.6694000029563903 (Best: 0.6746666669845581)\n",
      "Avg Val Loss: 0.8522651535272598, Avg Val Acc: 0.6017333328723907 (Best: 0.6746666669845581)\n",
      "=== Epoch: 25 ===\n",
      "Avg Train Loss: 0.8745405223965644, Avg Train Acc: 0.6742000019550324 (Best: 0.6746666669845581)\n",
      "Avg Val Loss: 0.852904160618782, Avg Val Acc: 0.6274666646122933 (Best: 0.6746666669845581)\n",
      "=== Epoch: 26 ===\n",
      "Avg Train Loss: 0.8982382741570473, Avg Train Acc: 0.6665000033378601 (Best: 0.6746666669845581)\n",
      "Avg Val Loss: 0.8804162096977234, Avg Val Acc: 0.6201333320140838 (Best: 0.6746666669845581)\n",
      "=== Epoch: 27 ===\n",
      "Avg Train Loss: 0.8851797613501549, Avg Train Acc: 0.6704999974370003 (Best: 0.6746666669845581)\n",
      "Avg Val Loss: 0.8692964023351669, Avg Val Acc: 0.613733332157135 (Best: 0.6746666669845581)\n",
      "=== Epoch: 28 ===\n",
      "Avg Train Loss: 0.8555019327998161, Avg Train Acc: 0.6823000001907349 (Best)\n",
      "Avg Val Loss: 0.8751297265291214, Avg Val Acc: 0.6185333320498466 (Best: 0.6746666669845581)\n",
      "=== Epoch: 29 ===\n",
      "Avg Train Loss: 0.8652930265665054, Avg Train Acc: 0.6728000012040138 (Best: 0.6746666669845581)\n",
      "Avg Val Loss: 0.8432502239942551, Avg Val Acc: 0.6145333340764045 (Best: 0.6746666669845581)\n",
      "=== Epoch: 30 ===\n",
      "Avg Train Loss: 0.8303494641184807, Avg Train Acc: 0.6855999988317489 (Best)\n",
      "Avg Val Loss: 0.8784392505884171, Avg Val Acc: 0.6122666648030282 (Best: 0.6746666669845581)\n",
      "=== Epoch: 31 ===\n",
      "Avg Train Loss: 0.866070884168148, Avg Train Acc: 0.6736000013351441 (Best: 0.6746666669845581)\n",
      "Avg Val Loss: 0.8681452280282974, Avg Val Acc: 0.6221333324909211 (Best: 0.6746666669845581)\n",
      "=== Epoch: 32 ===\n",
      "Avg Train Loss: 0.8953876772522926, Avg Train Acc: 0.668500000834465 (Best: 0.6746666669845581)\n",
      "Avg Val Loss: 0.8677021497488022, Avg Val Acc: 0.6236000031232833 (Best: 0.6746666669845581)\n",
      "=== Epoch: 33 ===\n",
      "Avg Train Loss: 0.8683510360121727, Avg Train Acc: 0.6774000024795532 (Best)\n",
      "Avg Val Loss: 0.8776423394680023, Avg Val Acc: 0.6207999992370605 (Best: 0.6746666669845581)\n",
      "=== Epoch: 34 ===\n",
      "Avg Train Loss: 0.8564930871129036, Avg Train Acc: 0.6726999992132187 (Best: 0.6746666669845581)\n",
      "Avg Val Loss: 0.8841843664646148, Avg Val Acc: 0.6258666661381721 (Best: 0.6746666669845581)\n",
      "=== Epoch: 35 ===\n",
      "Avg Train Loss: 0.8587491068243981, Avg Train Acc: 0.6781000000238419 (Best)\n",
      "Avg Val Loss: 0.8770749247074128, Avg Val Acc: 0.5953333342075348 (Best: 0.6746666669845581)\n",
      "=== Epoch: 36 ===\n",
      "Avg Train Loss: 0.8506629607081413, Avg Train Acc: 0.6753000018000602 (Best)\n",
      "Avg Val Loss: 0.8942889803647995, Avg Val Acc: 0.6186666637659073 (Best: 0.6746666669845581)\n",
      "=== Epoch: 37 ===\n",
      "Avg Train Loss: 0.8447061875462532, Avg Train Acc: 0.6824000012874604 (Best)\n",
      "Avg Val Loss: 0.8613789188861847, Avg Val Acc: 0.6190666657686233 (Best: 0.6746666669845581)\n",
      "=== Epoch: 38 ===\n",
      "Avg Train Loss: 0.8369659423828125, Avg Train Acc: 0.6880000007152557 (Best)\n",
      "Avg Val Loss: 0.8715554314851761, Avg Val Acc: 0.6517333304882049 (Best: 0.6746666669845581)\n",
      "=== Epoch: 39 ===\n",
      "Avg Train Loss: 0.8442062479257584, Avg Train Acc: 0.6861999991536141 (Best)\n",
      "Avg Val Loss: 0.8525782996416091, Avg Val Acc: 0.6724000012874604 (Best: 0.6746666669845581)\n",
      "=== Epoch: 40 ===\n",
      "Avg Train Loss: 0.842952454984188, Avg Train Acc: 0.6811999994516372 (Best)\n",
      "Avg Val Loss: 0.8567236363887787, Avg Val Acc: 0.6037333285808564 (Best: 0.6746666669845581)\n",
      "=== Epoch: 41 ===\n",
      "Avg Train Loss: 0.8564732348918915, Avg Train Acc: 0.6792000007629394 (Best)\n",
      "Avg Val Loss: 0.8335957443714141, Avg Val Acc: 0.6451999962329864 (Best: 0.6746666669845581)\n",
      "=== Epoch: 42 ===\n",
      "Avg Train Loss: 0.8219751253724098, Avg Train Acc: 0.6964000004529953 (Best)\n",
      "Avg Val Loss: 0.842789853811264, Avg Val Acc: 0.6231999957561493 (Best: 0.6746666669845581)\n",
      "=== Epoch: 43 ===\n",
      "Avg Train Loss: 0.870445088148117, Avg Train Acc: 0.6765000009536744 (Best)\n",
      "Avg Val Loss: 0.8915698283910751, Avg Val Acc: 0.6371999984979629 (Best: 0.6746666669845581)\n",
      "=== Epoch: 44 ===\n",
      "Avg Train Loss: 0.8229768905043602, Avg Train Acc: 0.6924999964237213 (Best)\n",
      "Avg Val Loss: 0.8740060597658157, Avg Val Acc: 0.6256000003218651 (Best: 0.6746666669845581)\n",
      "=== Epoch: 45 ===\n",
      "Avg Train Loss: 0.8145730969309807, Avg Train Acc: 0.6943999999761581 (Best)\n",
      "Avg Val Loss: 0.8702280437946319, Avg Val Acc: 0.640266664326191 (Best: 0.6746666669845581)\n",
      "=== Epoch: 46 ===\n",
      "Avg Train Loss: 0.8154001152515411, Avg Train Acc: 0.6973000001907349 (Best)\n",
      "Avg Val Loss: 0.8573676300048828, Avg Val Acc: 0.6401333349943161 (Best: 0.6746666669845581)\n",
      "=== Epoch: 47 ===\n",
      "Avg Train Loss: 0.8227619588375091, Avg Train Acc: 0.6908000022172928 (Best)\n",
      "Avg Val Loss: 0.8494274079799652, Avg Val Acc: 0.6653333348035813 (Best: 0.6746666669845581)\n",
      "=== Epoch: 48 ===\n",
      "Avg Train Loss: 0.7728010731935501, Avg Train Acc: 0.703999999165535 (Best)\n",
      "Avg Val Loss: 0.8780626785755158, Avg Val Acc: 0.6526666674017906 (Best: 0.6746666669845581)\n",
      "=== Epoch: 49 ===\n",
      "Avg Train Loss: 0.8125610542297363, Avg Train Acc: 0.6984000021219253 (Best)\n",
      "Avg Val Loss: 0.8753393584489823, Avg Val Acc: 0.6558666661381721 (Best: 0.6746666669845581)\n",
      "=== Epoch: 50 ===\n",
      "Avg Train Loss: 0.834919199347496, Avg Train Acc: 0.691100001335144 (Best)\n",
      "Avg Val Loss: 0.9008536618947983, Avg Val Acc: 0.6133333313465118 (Best: 0.6746666669845581)\n",
      "=== Epoch: 51 ===\n",
      "Avg Train Loss: 0.8131024971604347, Avg Train Acc: 0.6957000029087067 (Best)\n",
      "Avg Val Loss: 0.9133309805393219, Avg Val Acc: 0.6129333332180977 (Best: 0.6746666669845581)\n",
      "=== Epoch: 52 ===\n",
      "Avg Train Loss: 0.8277066662907601, Avg Train Acc: 0.6892000013589858 (Best)\n",
      "Avg Val Loss: 0.8798650056123734, Avg Val Acc: 0.637200001180172 (Best: 0.6746666669845581)\n",
      "=== Epoch: 53 ===\n",
      "Avg Train Loss: 0.8237293821573257, Avg Train Acc: 0.6899999988079071 (Best)\n",
      "Avg Val Loss: 0.8570600187778473, Avg Val Acc: 0.624666668176651 (Best: 0.6746666669845581)\n",
      "=== Epoch: 54 ===\n",
      "Avg Train Loss: 0.8104758813977242, Avg Train Acc: 0.6962999987602234 (Best)\n",
      "Avg Val Loss: 0.8897735357284546, Avg Val Acc: 0.6064000004529952 (Best: 0.6746666669845581)\n",
      "=== Epoch: 55 ===\n",
      "Avg Train Loss: 0.8198614808917045, Avg Train Acc: 0.6943999987840652 (Best)\n",
      "Avg Val Loss: 0.8688111954927444, Avg Val Acc: 0.6036000046133995 (Best: 0.6746666669845581)\n",
      "=== Epoch: 56 ===\n",
      "Avg Train Loss: 0.8406439033150673, Avg Train Acc: 0.6870000022649765 (Best)\n",
      "Avg Val Loss: 0.8456476879119873, Avg Val Acc: 0.6251999992132187 (Best: 0.6746666669845581)\n",
      "=== Epoch: 57 ===\n",
      "Avg Train Loss: 0.8170689815282821, Avg Train Acc: 0.7004000025987626 (Best)\n",
      "Avg Val Loss: 0.8769150006771088, Avg Val Acc: 0.6158666661381722 (Best: 0.6746666669845581)\n",
      "=== Epoch: 58 ===\n",
      "Avg Train Loss: 0.8136801660060883, Avg Train Acc: 0.6958999997377395 (Best)\n",
      "Avg Val Loss: 0.9083093202114105, Avg Val Acc: 0.6174666666984558 (Best: 0.6746666669845581)\n",
      "=== Epoch: 59 ===\n",
      "Avg Train Loss: 0.8353542023897171, Avg Train Acc: 0.6829999986290932 (Best)\n",
      "Avg Val Loss: 0.8861997044086456, Avg Val Acc: 0.6399999997019767 (Best: 0.6746666669845581)\n",
      "=== Epoch: 60 ===\n",
      "Avg Train Loss: 0.8347116723656655, Avg Train Acc: 0.6846999996900558 (Best)\n",
      "Avg Val Loss: 0.8855200415849686, Avg Val Acc: 0.6534666681289673 (Best: 0.6746666669845581)\n",
      "=== Epoch: 61 ===\n",
      "Avg Train Loss: 0.790990962088108, Avg Train Acc: 0.7044999998807907 (Best)\n",
      "Avg Val Loss: 0.8574697375297546, Avg Val Acc: 0.6638666677474976 (Best: 0.6746666669845581)\n",
      "=== Epoch: 62 ===\n",
      "Avg Train Loss: 0.8089027380943299, Avg Train Acc: 0.6956000024080277 (Best)\n",
      "Avg Val Loss: 0.8640136295557022, Avg Val Acc: 0.6605333358049392 (Best: 0.6746666669845581)\n",
      "=== Epoch: 63 ===\n",
      "Avg Train Loss: 0.7762211841344834, Avg Train Acc: 0.7114000010490418 (Best)\n",
      "Avg Val Loss: 0.8823371082544327, Avg Val Acc: 0.6273333337903023 (Best: 0.6746666669845581)\n",
      "=== Epoch: 64 ===\n",
      "Avg Train Loss: 0.7773523020744324, Avg Train Acc: 0.7024999994039536 (Best)\n",
      "Avg Val Loss: 0.8591298031806945, Avg Val Acc: 0.6566666668653488 (Best: 0.6746666669845581)\n",
      "=== Epoch: 65 ===\n",
      "Avg Train Loss: 0.8531192234158516, Avg Train Acc: 0.6852000007033348 (Best)\n",
      "Avg Val Loss: 0.8930421829223633, Avg Val Acc: 0.6164000019431114 (Best: 0.6746666669845581)\n",
      "=== Epoch: 66 ===\n",
      "Avg Train Loss: 0.7884966498613357, Avg Train Acc: 0.705900001525879 (Best)\n",
      "Avg Val Loss: 0.893224550485611, Avg Val Acc: 0.6341333302855492 (Best: 0.6746666669845581)\n",
      "=== Epoch: 67 ===\n",
      "Avg Train Loss: 0.810826495885849, Avg Train Acc: 0.6947000008821488 (Best)\n",
      "Avg Val Loss: 0.8616181963682175, Avg Val Acc: 0.6302666658163071 (Best: 0.6746666669845581)\n",
      "=== Epoch: 68 ===\n",
      "Avg Train Loss: 0.7765648183226586, Avg Train Acc: 0.7083000016212463 (Best)\n",
      "Avg Val Loss: 0.8901537036895752, Avg Val Acc: 0.6353333345055581 (Best: 0.6746666669845581)\n",
      "=== Epoch: 69 ===\n",
      "Avg Train Loss: 0.7933213356137275, Avg Train Acc: 0.7013000011444092 (Best)\n",
      "Avg Val Loss: 0.8818793082237244, Avg Val Acc: 0.6474666658043862 (Best: 0.6746666669845581)\n",
      "=== Epoch: 70 ===\n",
      "Avg Train Loss: 0.7954036459326744, Avg Train Acc: 0.6986999994516373 (Best)\n",
      "Avg Val Loss: 0.8822822934389114, Avg Val Acc: 0.6337333309650421 (Best: 0.6746666669845581)\n",
      "=== Epoch: 71 ===\n",
      "Avg Train Loss: 0.78255586206913, Avg Train Acc: 0.705100000500679 (Best)\n",
      "Avg Val Loss: 0.8917455583810806, Avg Val Acc: 0.6174666655063629 (Best: 0.6746666669845581)\n",
      "=== Epoch: 72 ===\n",
      "Avg Train Loss: 0.8026608824729919, Avg Train Acc: 0.7078000009059906 (Best)\n",
      "Avg Val Loss: 0.8812845999002457, Avg Val Acc: 0.6207999968528748 (Best: 0.6746666669845581)\n",
      "=== Epoch: 73 ===\n",
      "Avg Train Loss: 0.7953161662817001, Avg Train Acc: 0.7072000032663346 (Best)\n",
      "Avg Val Loss: 0.8907324230670929, Avg Val Acc: 0.6294666662812233 (Best: 0.6746666669845581)\n",
      "=== Epoch: 74 ===\n",
      "Avg Train Loss: 0.7986289206147194, Avg Train Acc: 0.7033999991416932 (Best)\n",
      "Avg Val Loss: 0.8723531538248062, Avg Val Acc: 0.6327999985218048 (Best: 0.6746666669845581)\n",
      "=== Epoch: 75 ===\n",
      "Avg Train Loss: 0.806800525188446, Avg Train Acc: 0.6958999994397164 (Best)\n",
      "Avg Val Loss: 0.8914091730117798, Avg Val Acc: 0.6324000018835068 (Best: 0.6746666669845581)\n",
      "=== Epoch: 76 ===\n",
      "Avg Train Loss: 0.7973668572306633, Avg Train Acc: 0.7004000043869019 (Best)\n",
      "Avg Val Loss: 0.8768244510889054, Avg Val Acc: 0.6177333331108094 (Best: 0.6746666669845581)\n",
      "=== Epoch: 77 ===\n",
      "Avg Train Loss: 0.841496333181858, Avg Train Acc: 0.6911000019311905 (Best)\n",
      "Avg Val Loss: 0.8829108256101609, Avg Val Acc: 0.6290666636824608 (Best: 0.6746666669845581)\n",
      "=== Epoch: 78 ===\n",
      "Avg Train Loss: 0.8046035611629486, Avg Train Acc: 0.7003000003099441 (Best)\n",
      "Avg Val Loss: 0.893103529214859, Avg Val Acc: 0.6317333352565765 (Best: 0.6746666669845581)\n",
      "=== Epoch: 79 ===\n",
      "Avg Train Loss: 0.7615347452461719, Avg Train Acc: 0.7154999989271164 (Best)\n",
      "Avg Val Loss: 0.9025313240289689, Avg Val Acc: 0.6166666674613953 (Best: 0.6746666669845581)\n",
      "=== Epoch: 80 ===\n",
      "Avg Train Loss: 0.8397504624724388, Avg Train Acc: 0.6929000014066696 (Best)\n",
      "Avg Val Loss: 0.876205306649208, Avg Val Acc: 0.6435999995470048 (Best: 0.6746666669845581)\n",
      "=== Epoch: 81 ===\n",
      "Avg Train Loss: 0.7914114624261857, Avg Train Acc: 0.7002999991178512 (Best)\n",
      "Avg Val Loss: 0.8970626497268677, Avg Val Acc: 0.6246666678786278 (Best: 0.6746666669845581)\n",
      "=== Epoch: 82 ===\n",
      "Avg Train Loss: 0.8111885243654251, Avg Train Acc: 0.7022000002861023 (Best)\n",
      "Avg Val Loss: 0.8724704849720001, Avg Val Acc: 0.6231999999284744 (Best: 0.6746666669845581)\n",
      "=== Epoch: 83 ===\n",
      "Avg Train Loss: 0.7769255071878434, Avg Train Acc: 0.7069999986886978 (Best)\n",
      "Avg Val Loss: 0.9074362498521805, Avg Val Acc: 0.6234666666388512 (Best: 0.6746666669845581)\n",
      "=== Epoch: 84 ===\n",
      "Avg Train Loss: 0.7850634843111038, Avg Train Acc: 0.715200001001358 (Best)\n",
      "Avg Val Loss: 0.8796837395429611, Avg Val Acc: 0.6499999970197677 (Best: 0.6746666669845581)\n",
      "=== Epoch: 85 ===\n",
      "Avg Train Loss: 0.7539841967821121, Avg Train Acc: 0.7209000009298324 (Best)\n",
      "Avg Val Loss: 0.8719890052080155, Avg Val Acc: 0.6485333320498466 (Best: 0.6746666669845581)\n",
      "=== Epoch: 86 ===\n",
      "Avg Train Loss: 0.7776707279682159, Avg Train Acc: 0.7046999990940094 (Best)\n",
      "Avg Val Loss: 0.8901648688316345, Avg Val Acc: 0.6305333322286606 (Best: 0.6746666669845581)\n",
      "=== Epoch: 87 ===\n",
      "Avg Train Loss: 0.7820924761891365, Avg Train Acc: 0.713999997973442 (Best)\n",
      "Avg Val Loss: 0.8777984547615051, Avg Val Acc: 0.6458666643500328 (Best: 0.6746666669845581)\n",
      "=== Epoch: 88 ===\n",
      "Avg Train Loss: 0.8109362709522248, Avg Train Acc: 0.701800000667572 (Best)\n",
      "Avg Val Loss: 0.9003121131658554, Avg Val Acc: 0.6501333355903626 (Best: 0.6746666669845581)\n",
      "=== Epoch: 89 ===\n",
      "Avg Train Loss: 0.8362000441551208, Avg Train Acc: 0.6903000015020371 (Best)\n",
      "Avg Val Loss: 0.8740448039770127, Avg Val Acc: 0.642133333683014 (Best: 0.6746666669845581)\n",
      "=== Epoch: 90 ===\n",
      "Avg Train Loss: 0.789661041200161, Avg Train Acc: 0.708400000333786 (Best)\n",
      "Avg Val Loss: 0.8987930053472519, Avg Val Acc: 0.6309333324432373 (Best: 0.6746666669845581)\n",
      "=== Epoch: 91 ===\n",
      "Avg Train Loss: 0.8025313112139701, Avg Train Acc: 0.6988999980688095 (Best)\n",
      "Avg Val Loss: 0.8802625101804733, Avg Val Acc: 0.6371999979019165 (Best: 0.6746666669845581)\n",
      "=== Epoch: 92 ===\n",
      "Avg Train Loss: 0.7909367543458938, Avg Train Acc: 0.7031000024080276 (Best)\n",
      "Avg Val Loss: 0.8829965627193451, Avg Val Acc: 0.6406666675209999 (Best: 0.6746666669845581)\n",
      "=== Epoch: 93 ===\n",
      "Avg Train Loss: 0.8185265392065049, Avg Train Acc: 0.6951999992132187 (Best)\n",
      "Avg Val Loss: 0.8698776692152024, Avg Val Acc: 0.6482666653394699 (Best: 0.6746666669845581)\n",
      "=== Epoch: 94 ===\n",
      "Avg Train Loss: 0.7862721338868142, Avg Train Acc: 0.7058000004291535 (Best)\n",
      "Avg Val Loss: 0.8833567744493485, Avg Val Acc: 0.6461333322525025 (Best: 0.6746666669845581)\n",
      "=== Epoch: 95 ===\n",
      "Avg Train Loss: 0.7669743791222572, Avg Train Acc: 0.7113999992609024 (Best)\n",
      "Avg Val Loss: 0.895469890832901, Avg Val Acc: 0.6447999992966652 (Best: 0.6746666669845581)\n",
      "=== Epoch: 96 ===\n",
      "Avg Train Loss: 0.809481893479824, Avg Train Acc: 0.7013999998569489 (Best)\n",
      "Avg Val Loss: 0.8752950322628021, Avg Val Acc: 0.6350666654109954 (Best: 0.6746666669845581)\n",
      "=== Epoch: 97 ===\n",
      "Avg Train Loss: 0.7479413467645645, Avg Train Acc: 0.7237999993562698 (Best)\n",
      "Avg Val Loss: 0.8678381890058517, Avg Val Acc: 0.6254666650295257 (Best: 0.6746666669845581)\n",
      "=== Epoch: 98 ===\n",
      "Avg Train Loss: 0.7387892150878906, Avg Train Acc: 0.7248999965190888 (Best)\n",
      "Avg Val Loss: 0.874118047952652, Avg Val Acc: 0.6519999992847443 (Best: 0.6746666669845581)\n",
      "=== Epoch: 99 ===\n",
      "Avg Train Loss: 0.7716983312368393, Avg Train Acc: 0.710900000333786 (Best)\n",
      "Avg Val Loss: 0.8839045339822769, Avg Val Acc: 0.6478666678071022 (Best: 0.6746666669845581)\n"
     ]
    }
   ],
   "source": [
    "if detector.train:\n",
    "    detector.trainModel(train_X=train_X, train_Y = train_y, val_X=val_X, val_Y=val_y, save_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Acc: 0.6210266673713922\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6210266673713922"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detector.testModel(test_X=test_X, test_Y=test_y)\n",
    "# , modelName=f\"{detector.model_name}_check_point\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# machine learning\n",
    "\n",
    "X = test_X\n",
    "y = test_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from itertools import combinations\n",
    "\n",
    "def run_experiment(X, y, n_experiments=100):\n",
    "    classes = np.unique(y)\n",
    "    \n",
    "    accuracies = []\n",
    "    \n",
    "    for _ in range(n_experiments):\n",
    "        # 隨機選擇5個類別\n",
    "        selected_classes = np.random.choice(classes, 5, replace=False)\n",
    "        \n",
    "        train_indices = []\n",
    "        test_indices = []\n",
    "        \n",
    "        for cls in selected_classes:\n",
    "            cls_indices = np.where(y == cls)[0]\n",
    "            train_indices.extend(np.random.choice(cls_indices, 5, replace=False))\n",
    "            test_indices.extend([idx for idx in cls_indices if idx not in train_indices])\n",
    "\n",
    "        X_train, y_train = X[train_indices], y[train_indices]\n",
    "        X_test, y_test = X[test_indices], y[test_indices]\n",
    "\n",
    "        # 訓練SVM\n",
    "        svm = SVC(kernel='rbf', gamma='scale')\n",
    "        svm.fit(X_train, y_train)\n",
    "        \n",
    "        # 預測並計算準確率\n",
    "        y_pred = svm.predict(X_test)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        accuracies.append(acc)\n",
    "    \n",
    "    return np.mean(accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average 5-way 5-shot accuracy: 0.7388\n"
     ]
    }
   ],
   "source": [
    "avg_accuracy = run_experiment(X, y)\n",
    "print(f\"Average 5-way 5-shot accuracy: {avg_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "byteSequence",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
