#!/usr/bin/env python
# -*- coding: UTF-8 -*-

import sys
import math
import logging
import numpy as np
import pandas as pd
from malwareDetector import malwareDetector
from math import sqrt
from abc import ABCMeta, abstractmethod
import numpy as np
import numpy.linalg as linalg
import os
import pickle
from plot import plot
from sklearn.manifold import TSNE
from sklearn.cluster import KMeans

logger = logging.getLogger("dpc_cluster")

def get_cluster_distance(dataNp: np.array, y: np.array, label_mapping: dict, dataDf: pd.DataFrame, detector: malwareDetector, filePrefix: str) -> None:
	'''
	Get cluster distance

	Args:
		dataNp       : data in numpy array
		y            : label of data
		label_mapping: label mapping
		dataDf       : data in pandas dataframe
		detector     : malwareDetector object
		filePrefix   : file prefix for cluster data
	'''
	if not os.path.exists(f"{detector.clusterDataFolder}/{detector.cpuArch}"):
		detector.mkdir(f"{detector.clusterDataFolder}/{detector.cpuArch}")
	if not os.path.exists(f"{detector.clusterResultFolder}/{detector.cpuArch}"):
		detector.mkdir(f"{detector.clusterResultFolder}/{detector.cpuArch}")

	for i in label_mapping.keys():
		print(f"Processing {label_mapping[i]}")
		family = label_mapping[i]
		familyData = dataNp[y == i]
		dataTxtOutputPath = f"{detector.clusterDataFolder}/{detector.cpuArch}/{filePrefix}_{family}.txt"
		with open(dataTxtOutputPath, 'w') as f:
			for i in range(len(familyData)):
				f.write('\t'.join(map(str, familyData[i])) + "\n")
				
			f.close()
		builder = DistanceBuilder()
		dataDatOutputPath = f"{detector.clusterDataFolder}/{detector.cpuArch}/{filePrefix}_{family}.dat"
		builder.load_points(dataTxtOutputPath)
		builder.build_distance_file_for_cluster(SqrtDistance(), dataDatOutputPath)


	print("Every family data has been processed.")

def get_family_index(dataDf: pd.DataFrame, label_mapping: dict, detector: malwareDetector, filePrefix: str) -> None:
	'''
	Get family index

	Args:
		dataDf       : data in pandas dataframe
		label_mapping: label mapping
		detector     : malwareDetector object
		filePrefix   : file prefix for cluster data
	'''
	if not os.path.exists(f"{detector.clusterDataFolder}/{detector.cpuArch}"):
		detector.mkdir(f"{detector.clusterDataFolder}/{detector.cpuArch}")
	if not os.path.exists(f"{detector.clusterResultFolder}/{detector.cpuArch}"):
		detector.mkdir(f"{detector.clusterResultFolder}/{detector.cpuArch}")

	for i in label_mapping.keys():
		family = label_mapping[i]
		familyDataDf = dataDf.loc[dataDf["family"] == family]
		familyIndex = familyDataDf.index
		indexOutputPath = f"{detector.clusterDataFolder}/{detector.cpuArch}/{filePrefix}_{family}_index.pickle"
		with open(indexOutputPath, 'wb') as f:
			pickle.dump(familyIndex, f)
			f.close()
	print("Every family index has been processed.")


def get_cluster_result(label_mapping: dict, dataDf: pd.DataFrame, detector: malwareDetector, filePrefix: str, plot: plot, colunm: str, dfSavingPath: str) -> None:
	'''
	Get cluster result in different family

	Args:
		label_mapping: label mapping
		dataDf       : data in pandas dataframe
		detector     : malwareDetector object
		filePrefix   : file prefix for cluster data
		plot         : plot object
		colunm       : column name for cluster result
		dfSavingPath : path to save the cluster result

	Returns:
		dataDf with cluster result
	'''

	if not os.path.exists(f"{detector.picFolder}/{detector.cpuArch}"):
		detector.mkdir(f"{detector.picFolder}/{detector.cpuArch}")

	for i in label_mapping.keys():
		family = label_mapping[i]
		title = f"{filePrefix}_{family}"
		data = f"{detector.clusterDataFolder}/{detector.cpuArch}/{filePrefix}_{family}.dat"
		logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
		dpcluster = DensityPeakCluster()
		try:
			auto_select_dc = False
			distances, max_dis, min_dis, max_id, rho, rc = dpcluster.local_density(load_paperdata, data, auto_select_dc = auto_select_dc)
		except:
			auto_select_dc = True
			distances, max_dis, min_dis, max_id, rho, rc = dpcluster.local_density(load_paperdata, data, auto_select_dc = auto_select_dc)
		delta, nneigh = min_distance(max_id, max_dis, distances, rho)
		if plot.bool:
			plot.plot_rho_delta(rho, delta, title)
			plot.plot_rhodelta_rho(rho,delta, title)
		indexPath = f"{detector.clusterDataFolder}/{detector.cpuArch}/{filePrefix}_{family}_index.pickle"
		with open(indexPath, 'rb') as f:
			familyIndex = pickle.load(f)
			f.close()
		familyData = dataDf.loc[familyIndex]
		clusterNum = len(set(familyData["CPU"].values))
		print(f"CPU architecture number of {family} is {clusterNum}")
		rhodelta = rho * delta
		order = np.argsort(-rhodelta)
		density_threshold = np.min(rho[order[:clusterNum]])
		distance_threshold = np.min(delta[order[:clusterNum]])
		print(f"Threshold of {family}: density {density_threshold}, distance {distance_threshold}")
		rho, delta, nneigh = dpcluster.cluster(load_paperdata, data, density_threshold, distance_threshold, auto_select_dc = auto_select_dc)
		logger.info(str(len(dpcluster.ccenter)) + ' center as below')
		for idx, center in dpcluster.ccenter.items():
			logger.info('%d %f %f' %(idx, rho[center], delta[center]))
		plot.plot_cluster(dpcluster, title)
		dataDf = write_cluster_result(dataDf, detector, family, indexPath, f"{detector.clusterResultFolder}/{detector.cpuArch}/{filePrefix}_{family}_cluster.txt", colunm)
	dataDf.to_csv(f"{dfSavingPath}/vectorzieTrain.csv", index = False)	
	return dataDf
def write_cluster_result(dataDf: pd.DataFrame, detector: malwareDetector, family: str, indexPath: str, resultPath: str, colunm: str) -> None:

	with open (indexPath, 'rb') as f:
		familyIndex = pickle.load(f)
		f.close()

	with open(resultPath, 'r') as f:
		clusterResult = f.read().splitlines()
		f.close()
	unique, counts = np.unique(clusterResult, return_counts=True)
	for i in range(len(clusterResult)):
		# index of clusterResult[i] in unique
		temp = family + "_" + str(np.where(unique == clusterResult[i])[0][0])
		dataDf.loc[familyIndex[i], colunm] = temp
	return dataDf
	

def load_paperdata(distance_f):
	'''
	Load distance from data

	Args:
		distance_f : distance file, the format is column1-index 1, column2-index 2, column3-distance
	
	Returns:
	    distances dict, max distance, min distance, max continues id
	'''
	logger.info("PROGRESS: load data")
	distances = {}
	min_dis, max_dis = sys.float_info.max, 0.0
	max_id = 0
	with open(distance_f, 'r') as fp:
		for line in fp:
			x1, x2, d = line.strip().split(' ')
			x1, x2 = int(x1), int(x2)
			max_id = max(max_id, x1, x2)
			dis = float(d)
			min_dis, max_dis = min(min_dis, dis), max(max_dis, dis)
			distances[(x1, x2)] = float(d)
			distances[(x2, x1)] = float(d)
	for i in range(max_id):
		distances[(i, i)] = 0.0
	logger.info("PROGRESS: load end")

	return distances, max_dis, min_dis, max_id

def select_dc(max_id, max_dis, min_dis, distances, auto = False):
	'''
	Select the local density threshold, default is the method used in paper, auto is `autoselect_dc`

	Args:
		max_id    : max continues id
		max_dis   : max distance for all points
		min_dis   : min distance for all points
		distances : distance dict
		auto      : use auto dc select or not
	
	Returns:
	    dc that local density threshold
	'''
	logger.info("PROGRESS: select dc")
	if auto:
		return autoselect_dc(max_id, max_dis, min_dis, distances)
	percent = 2.0
	position = int(max_id * (max_id + 1) / 2 * percent / 100)
	dc = sorted(distances.values())[position * 2 + max_id]
	logger.info("PROGRESS: dc - " + str(dc))
	return dc

def autoselect_dc(max_id, max_dis, min_dis, distances):
	'''
	Auto select the local density threshold that let average neighbor is 1-2 percent of all nodes.

	Args:
		max_id    : max continues id
		max_dis   : max distance for all points
		min_dis   : min distance for all points
		distances : distance dict
	
	Returns:
	    dc that local density threshold
	'''
	dc = (max_dis + min_dis) / 2

	while True:
		nneighs = sum([1 for v in distances.values() if v < dc]) / max_id ** 2
		if nneighs >= 0.01 and nneighs <= 0.002:
			break
		# binary search
		if nneighs < 0.01:
			min_dis = dc
		else:
			max_dis = dc
		dc = (max_dis + min_dis) / 2
		if max_dis - min_dis < 0.0001:
			break
	return dc

def local_density(max_id, distances, dc, guass=True, cutoff=False):
	'''
	Compute all points' local density

	Args:
		max_id    : max continues id
		distances : distance dict
		gauss     : use guass func or not(can't use together with cutoff)
		cutoff    : use cutoff func or not(can't use together with guass)
	
	Returns:
	    local density vector that index is the point index that start from 1
	'''
	assert guass and cutoff == False and guass or cutoff == True
	logger.info("PROGRESS: compute local density")
	guass_func = lambda dij, dc : math.exp(- (dij / dc) ** 2)
	cutoff_func = lambda dij, dc: 1 if dij < dc else 0
	func = guass and guass_func or cutoff_func
	rho = [-1] + [0] * max_id
	for i in range(1, max_id):
		for j in range(i + 1, max_id + 1):
			rho[i] += func(distances[(i, j)], dc)
			rho[j] += func(distances[(i, j)], dc)
		if i % (max_id / 10) == 0:
			logger.info("PROGRESS: at index #%i" % (i))
	return np.array(rho, np.float32)

def min_distance(max_id, max_dis, distances, rho):
	'''
	Compute all points' min distance to the higher local density point(which is the nearest neighbor)

	Args:
		max_id    : max continues id
		max_dis   : max distance for all points
		distances : distance dict
		rho       : local density vector that index is the point index that start from 1
	
	Returns:
	    min_distance vector, nearest neighbor vector
	'''
	logger.info("PROGRESS: compute min distance to nearest higher density neigh")
	sort_rho_idx = np.argsort(-rho)
	delta, nneigh = [0.0] + [float(max_dis)] * (len(rho) - 1), [0] * len(rho)
	delta[sort_rho_idx[0]] = -1.
	for i in range(1, max_id):
		for j in range(0, i):
			old_i, old_j = sort_rho_idx[i], sort_rho_idx[j]
			if distances[(old_i, old_j)] < delta[old_i]:
				delta[old_i] = distances[(old_i, old_j)]
				nneigh[old_i] = old_j
		if i % (max_id / 10) == 0:
			logger.info("PROGRESS: at index #%i" % (i))
	delta[sort_rho_idx[0]] = max(delta)
	return np.array(delta, np.float32), np.array(nneigh, np.float32)


class DensityPeakCluster(object):
	def local_density(self, load_func, distance_f, dc = None, auto_select_dc = False):
		'''
		Just compute local density

		Args:
		    load_func     : load func to load data
		    distance_f    : distance data file
		    dc            : local density threshold, call select_dc if dc is None
		    autoselect_dc : auto select dc or not

		Returns:
		    distances dict, max distance, min distance, max index, local density vector
		'''
		assert not (dc != None and auto_select_dc)
		distances, max_dis, min_dis, max_id = load_func(distance_f)
		if dc == None:
			dc = select_dc(max_id, max_dis, min_dis, distances, auto = auto_select_dc)
		rho = local_density(max_id, distances, dc)
		return distances, max_dis, min_dis, max_id, rho, dc

	def cluster(self, load_func, distance_f, density_threshold, distance_threshold, dc = None, auto_select_dc = False):
		'''
		Cluster the data

		Args:
		    load_func          : load func to load data
		    distance_f         : distance data file
		    dc                 : local density threshold, call select_dc if dc is None
		    density_threshold  : local density threshold for choosing cluster center
		    distance_threshold : min distance threshold for choosing cluster center
		    autoselect_dc      : auto select dc or not

		Returns:
		    local density vector, min_distance vector, nearest neighbor vector
		'''	
		assert not (dc != None and auto_select_dc)
		distances, max_dis, min_dis, max_id , rho , dc= self.local_density(load_func, distance_f, dc = dc, auto_select_dc = auto_select_dc)
		delta, nneigh = min_distance(max_id, max_dis, distances, rho)
		logger.info("PROGRESS: start cluster")
		cluster, ccenter = {}, {}  #cl/icl in cluster_dp.m

		for idx, (ldensity, mdistance, nneigh_item) in enumerate(zip(rho, delta, nneigh)):
			if idx == 0: continue

			if ldensity >= density_threshold and mdistance >= distance_threshold:
				ccenter[idx] = idx
				cluster[idx] = idx
			else:
				cluster[idx] = -1

		#assignation

		ordrho=np.argsort(-rho)

		for i in range(ordrho.shape[0]-1):
			if ordrho[i] == 0: continue
			if cluster[ordrho[i]] == -1:
				cluster[ordrho[i]] = cluster[nneigh[ordrho[i]]]
			if i % (max_id / 10) == 0:
				logger.info("PROGRESS: at index #%i" % (i))
		
		#halo
		halo, bord_rho = {},{}
		for i in range(1,ordrho.shape[0]):
			halo[i] = cluster[i]
		if len(ccenter) > 0:
			for idx in ccenter.keys():
				bord_rho[idx] = 0.0
			for i in range(1,rho.shape[0]-1):
				for j in range(i + 1,rho.shape[0]):
					if cluster[i] != cluster[j] and distances[i,j] <= dc:
						rho_aver = (rho[i] + rho[j]) / 2.0
						if rho_aver > bord_rho[cluster[i]]:
							bord_rho[cluster[i]] = rho_aver
						if rho_aver > bord_rho[cluster[j]]:
							bord_rho[cluster[j]] = rho_aver
			for i in range(1,rho.shape[0]):
				if rho[i] < bord_rho[cluster[i]]:
					halo[i] = 0
		for i in range(1,rho.shape[0]):
			if halo[i] == 0:
				cluster[i] =- 1

		self.cluster, self.ccenter = cluster, ccenter
		self.distances = distances
		self.max_id = max_id
		logger.info("PROGRESS: ended")
		return rho, delta, nneigh

class WrongVecError(Exception):
    '''
    Raised when an operation use empty or not same size vector.
    
    Attributes:
        value: error info
    '''
    def __init__(self, value):
        self.value = value
    
    def __str__(self):
        return repr(self.value)

class Distance():
  """
    abstract class, represent distance of two vector
    
    Attributes:
    """    

  __metaclass__ = ABCMeta
         
  @abstractmethod
  def distance(self, vec1, vec2):
    """
    Compute distance of two vector(one line numpy array)
    if you use scipy to store the sparse matrix, please use s.getrow(line_num).toarray() build the one dimensional array
    
    Args:
        vec1: the first line vector, an instance of array
        vec2: the second line vector, an instance of array
      
    Returns:
        the computed distance
    
    Raises:
        TypeError: if vec1 or vec2 is not numpy.ndarray and one line array
    """
    if not isinstance(vec1, np.ndarray) or not isinstance(vec2, np.ndarray):
      raise TypeError("type of vec1 or vec2 is not numpy.ndarray")
    if vec1.ndim != 1 or vec2.ndim != 1:
      raise WrongVecError("vec1 or vec2 is not one line array")
    if vec1.size != vec2.size:
      raise WrongVecError("vec1 or vec2 is not same size")    
    pass
  
class SqrtDistance(Distance):
  """
  Square distance
      
  a sub class of Distance
  """  

  def distance(self, vec1, vec2):
    """
    Compute distance of two vector by square distance
    """
    super(SqrtDistance, self).distance(vec1, vec2)      #super method
    vec=vec1-vec2
    return sqrt(sum([pow(item, 2) for item in vec]))

class DistanceBuilder(object):

  """
  Build distance file for cluster
  """  

  def __init__(self):
    self.vectors = []

  def load_points(self, filename):
    '''
    Load all points from file(x dimension vectors)

    Args:
        filename : file's name that contains all points. Format is a vector one line, each dimension value split by blank space
    '''
    with open(filename, 'r') as fp:
      for line in fp:
        self.vectors.append(np.array(list(map(float, line.split('\t')[:])), dtype = np.float32))
    self.vectors = np.array(self.vectors, dtype = np.float32)


  def build_distance_file_for_cluster(self, distance_obj, filename):
    '''
    Save distance and index into file

    Args:
        distance_obj : distance.Distance object for compute the distance of two point
        filename     : file to save the result for cluster
    '''
    fo = open(filename, 'w')
    for i in range(len(self.vectors) - 1):
      for j in range(i, len(self.vectors)):
        fo.write(str(i + 1) + ' ' + str(j + 1) + ' ' + str(distance_obj.distance(self.vectors[i], self.vectors[j])) + '\n')
    fo.close()

class kMeansTsne():
	"""
	kMeansTsne class, use kMeans
	"""
	def __init__(self, detector: malwareDetector):
		self.detector = detector
	
	def get_cluster_result(self, dataNp: np.array, y: np.array, label_mapping: dict, dataDf: pd.DataFrame, colunm: str, dfSavingPath: str) -> None:
		for i in label_mapping.keys():
			family = label_mapping[i]
			familyData = dataNp[y == i]
			familyDataDf = dataDf.loc[y == i]
			familyIndex = familyDataDf.index
			clusterNum = len(set(familyDataDf["CPU"].values))
			kmeans = KMeans(n_clusters = clusterNum, random_state = self.detector.seed).fit(familyData)
			for i in range(len(familyData)):
				dataDf.loc[familyIndex[i], colunm] = family + "_" + str(kmeans.labels_[i])
			print(f"CPU architecture number of {family} is {len(set(kmeans.labels_))}")
		dataDf.to_csv(f"{dfSavingPath}/vectorzieTrain.csv", index = False)	
		return dataDf
	
from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler
class dbscan():

	def __init__(self, detector: malwareDetector):
		self.detector = detector

	
	def get_cluster_result(self, dataNp: np.array, y: np.array, label_mapping: dict, dataDf: pd.DataFrame, colunm: str, dfSavingPath: str) -> None:
		for i in label_mapping.keys():
			family = label_mapping[i]
			familyData = dataNp[y == i]
			familyDataDf = dataDf.loc[y == i]
			familyIndex = familyDataDf.index
			clusterNum = len(set(familyDataDf["CPU"].values))
			X = StandardScaler().fit_transform(familyData)
			db = DBSCAN(eps=3, min_samples=3).fit(X)
			for i in range(len(familyData)):
				dataDf.loc[familyIndex[i], colunm] = family + "_" + str(db.labels_[i])
			print(f"CPU architecture number of {family} is {len(set(db.labels_))}")
		dataDf.to_csv(f"{dfSavingPath}/vectorzieTrain.csv", index = False)	
		return dataDf